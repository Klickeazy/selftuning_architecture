{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Numerical Experiments"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d10203177215c74"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Code Init"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "228829088d62fe49"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function file import\n",
    "import functionfile_speedygreedy as ff"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf9edc392b504428"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment Overview\n",
    "\n",
    "Experiment parameters are created in *experiment_parameters.csv* (works with MS Excel)\n",
    "\n",
    "### Parameters:\n",
    "- ***experiment_no*** : Ensure that every new row has a new number to prevent data overwrites\n",
    "- ***test_model*** and ***test_parameter***: These define the experiment model and control parameter\n",
    "\n",
    "| Exp No | test_model                                        | Test Description                                                                                                                                                                                                                               | *test_parameter* Effect                                                                                                                                                                                             |\n",
    "|:------:|:--------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "|  1.1   | *fixed_vs_self_tuning*                            | compare fixed architecture to self-tuning architecture for a generated network                                                                                                                                                                 | number of changes permitted for self_tuning architecture $N=$*test_parameter*                                                                                                                                       |\n",
    "|  1.2   | *statistics_fixed_vs_self_tuning*                 | run fixed_vs_self_tuning for 100 different randomly generated network graphs                                                                                                                                                                   | number of changes permitted for self_tuning architecture $N=$*test_parameter*                                                                                                                                       |\n",
    "|  1.3   | *statistics_pointdistribution_openloop*           | run fixed_vs_self_tuning over initial points on eigenvectors of open-loop dynamics matrix of a single instance of a randomly generated network                                                                                                 | number of changes permitted for self_tuning architecture $N=$*test_parameter*                                                                                                                                       |\n",
    "|  2.1   | *self_tuning_number_of_changes*                   | compare the effect of 1 vs $n$ number of architecture optimization changes allowed per simulation step                                                                                                                                         | number of changes permitted for self_tuning architecture $N\\rightarrow\\infty$ vs $N=$*test_parameter*                                                                                                               |\n",
    "|  2.2   | *statistics_self_tuning_number_of_changes*        | run self_tuning_number_of_changes for 100 different randomly generated network graphs                                                                                                                                                          | number of changes permitted for self_tuning architecture $N\\rightarrow\\infty$ vs $N=$*test_parameter*                                                                                                               |\n",
    "|  3.1   | *self_tuning_prediction_horizon*                  | compare the effect of scaling prediction time horizon                                                                                                                                                                                          | scales the base time horizon $T_p$ vs $T_p$ * *test_parameter*                                                                                                                                                      |\n",
    "|  3.2   | *statistics_self_tuning_prediction_horizon*       | run self_tuning_prediction_horizon for 100 different randomly generated network graphs                                                                                                                                                         | scales the base time horizon $T_p$ vs $T_p$ * *test_parameter*                                                                                                                                                      | \n",
    "|  4.1   | *self_tuning_architecture_cost_no_lim*            | compare the effect of scaling architecture running and switching costs with unbounded changes per swap iteration $N\\rightarrow\\infty$ and loose architecture constraints $\\mathcal{L}_m = \\mathcal{L}_{m'} < \\mathcal{L}_M = \\mathcal{L}_{M'}$ | scales the base architecture costs  $R_{2,\\mathcal{B}}, R_{2,\\mathcal{C}}, R_{3,\\mathcal{B}}, R_{3,\\mathcal{C}}$ vs *test_parameter* * $R_{2,\\mathcal{B}}, R_{2,\\mathcal{C}}, R_{3,\\mathcal{B}}, R_{3,\\mathcal{C}}$ |\n",
    "|  4.2   | *self_tuning_architecture_constraints*            | compare the effect of tight vs loose architecture constraints on self-tuning architecture under constant running + switching costs                                                                                                             | changes lower bound on active architecture sets $[\\mathcal{L}_{m}, \\mathcal{L}_{m'}]$ vs $[$ *test_parameter*, *test_parameter* $]$                                                                                 |\n",
    "|  4.3   | *statistics_self_tuning_architecture_cost_no_lim* | run *self_tuning_architecture_cost_no_lim* for 100 different randomly generated network graphs                                                                                                                                                 | scales the base architecture costs $R_{2,\\mathcal{B}}, R_{2,\\mathcal{C}}, R_{3,\\mathcal{B}}, R_{3,\\mathcal{C}}$ vs *test_parameter* * $R_{2,\\mathcal{B}}, R_{2,\\mathcal{C}}, R_{3,\\mathcal{B}}, R_{3,\\mathcal{C}}$  |\n",
    "\n",
    "- Network generation parameters:\n",
    "    - ***number_of_nodes*** : number of nodes in the network model\n",
    "    - ***network_model*** : type of network connection - generated using *numpy* and *networkx* packages\n",
    "        - *rand* : random weighted adjacency matrix with weights $[0,1)$\n",
    "        - *ER* : a realization of an Erdos-Renyi random graph generator with edge probability $p$\n",
    "        - *BA* : generate a realization of a Barabasi-Albert random graph generator with initial seed $p$\n",
    "        - *path* : generate a path graph\n",
    "        - *cycle* : generate a cycle graph\n",
    "        - *eval_squeeze* : generate spectrum of eigenvalues in $(-1-p, -1+p) \\cup (1-p, 1+p)$ to force stable modes with slow convergence and unstable modes with slow divergence\n",
    "        - *eval_bound* : generate spectrum of eigenvalues in $(-1, -1+p) \\cup (1-p, 1)$ to force stable modes with slow convergence\n",
    "        All graphs are checked to be well-connected, i.e., there are no isolated subnetworks and there is a path (direct or indirect) between every pair of nodes\n",
    "        For *eval_squeeze* and *eval_bound*, the underlying eigenvectors are randomly generated and normalized. All nodes have a self-connecting edge before uniform scaling is enforced \n",
    "    - ***network_parameter*** : generation parameter for ER, BA, eval_squeeze and eval_bound generators\n",
    "    - ***rho*** : uniform scaling parameter to apply to adjacency matrix if required\n",
    "    - ***second_order*** and ***second_order_network*** : specify if network nodes have second-order dynamics and which level is connected by the network adjacency $G_{adj}$\n",
    "        Dynamics of the $i^{th}$ node is given by $x'_{i,t}$ and $x_{i,t}$\n",
    "        - If not *second_order*, then *number_of_states* $n=$ *number_of_nodes*\n",
    "        - If *second_order*, then *number_of_states* $n=2$ * *number_of_nodes*\n",
    "        Open-loop dynamics based on *second_order_network* parameter for states $x_t = \\begin{bmatrix} \\dots x_{i,t} \\dots x'_{i,t} \\dots \\end{bmatrix}^\\top$: \n",
    "        - *second_order_network* = 1 : $A = \\begin{bmatrix} G_{adj} & \\mathbf{0} \\\\ \\mathbf{I} & \\mathbf{I} \\end{bmatrix}$ where network dynamics affects $x_{i,t}$ and $x'_{i,t+1} = x'_{i,t} + x_{i,t}$\n",
    "        - *second_order_network* = 2 : $A = \\begin{bmatrix} \\mathbf{I} & \\mathbf{0} \\\\ \\mathbf{I} & G_{adj} \\end{bmatrix}$ where network dynamics affects $x'_{i,t}$  and $x'_{i,t+1} = x'_{i,t} + x_{i,t} + \\sum_{j\\in\\mathscr{N}(x_i)}G_{adj,i,j}x_j$\n",
    "- Architecture parameters: Applicable for both actuators and sensors\n",
    "    Initializes available architecture as identity basis vectors of $\\mathbb{R}^n$ space\n",
    "    - ***initial_architecture_size*** : size of initial randomly placed or design-time optimized architecture $|S_{t=0}|, |S_{t=0}|$\n",
    "    - ***architecture_constraint_min***, ***architecture_constraint_max*** : min $\\mathcal{L}_m, \\mathcal{L}_{m'}$ and max $\\mathcal{L}_M, \\mathcal{L}_{M'}$ constraints on the size of the architecture sets. Set equal to each other for constrained optimization\n",
    "    - ***second_order_architecture*** : needs to be defined if second-order network dynamics for actuator inputs and sensor measurements\n",
    "        - *second_order_architecture* = 1 : architecture connected to $x_{i,t}$ nodes\n",
    "        - *second_order_architecture* = 2 : architecture connected to $x'_{i,t}$ nodes\n",
    "    - ***Q_cost_scaling***, ***R_cost_scaling*** : scaling costs on states and inputs for control objective - default 1 is identity\n",
    "        - $Q_\\mathcal{B} = \\mathbf{I}_{n}$ * *Q_cost_scaling*\n",
    "        - $R_{1,|\\mathcal{B}|} = \\mathbf{I}_{|\\mathcal{B}|}$ * *R_cost_scaling* : assumes submatrix $R_{1,|S_t|}$ of suitable dimension imposes identical input costs on all active actuators $S_t$\n",
    "    - ***B_run_cost***, ***C_run_cost*** : vector of uniform architecture running costs for actuators and sensors respectively\n",
    "        - Actuator running costs: $R_{2,\\mathcal{B}} = \\mathbf{1}_{|\\mathcal{B}|}$ * *B_run_cost*\n",
    "        - Sensor running costs: $R_{2,\\mathcal{C}} = \\mathbf{1}_{|\\mathcal{C}|}$ * *C_run_cost*\n",
    "    - ***B_switch_cost***, ***C_switch_cost*** : vector of uniform architecture switching costs for actuators and sensors respectively\n",
    "        - Actuator switching costs: $R_{3,\\mathcal{B}} = \\mathbf{1}_{|\\mathcal{B}|}$ * *B_switch_cost*\n",
    "        - Sensor switching costs: $R_{3,\\mathcal{C}} = \\mathbf{1}_{|\\mathcal{C}|}$ * *C_switch_cost*\n",
    "- Disturbance parameters: additive process/measurement noise and un-modelled disturbance generators\n",
    "    - ***W_scaling*** : scaling of identity matrix on covariance of zero-mean process noise\n",
    "    - ***V_scaling*** : scaling of identity matrix on covariance of zero-mean measurement noise - dimension affected by number of active sensors $|S'_t|$\n",
    "        Note that *W_scaling* and *V_scaling* are parameters of estimation optimization\n",
    "        - Process noise $w_t \\sim \\mathcal{N}(\\mathbf{0}_n, \\mathbf{I}_n$ * *W_scaling* $)$\n",
    "        - Measurement noise $v_t \\sim \\mathcal{N}(\\mathbf{0}_{|S'_t|}, \\mathbf{I}_{|S'_t|}$ * *V_scaling* $)$\n",
    "    - ***disturbance_model*** : type of un-modelled noise model\n",
    "        - *process* - additional noise only in process\n",
    "        - *measurement* - additional noise only in measurement\n",
    "        - *combined* - additional noise in both process and measurement\n",
    "    - ***disturbance_step***, ***disturbance_number***, ***disturbance_magnitude*** : how frequently, at which random states and how large the randomly generated un-modelled noise enters the process and measurements\n",
    "        - *process* noise $w_t$ for $n_k={n \\choose \\textit{disturbance_number}$ random states\n",
    "        - *measurement* noise $v_t$ for all active sensor outputs\n",
    "            $w_{t=i*\\textit{disturbance_step}, n_k} = \\pm \\textit{disturbance_magnitude}$\n",
    "            $v_{t=i*\\textit{disturbance_step}} = \\pm\\textit{disturbance_magnitude}$\n",
    "\n",
    "- Simulation parameters:\n",
    "    - ***prediction_time_horizon*** : $T_p$ length of the (MPC-like) receding prediction horizon over which architecture is assumed to be constant and optimized for\n",
    "    - ***X0_scaling*** : uniform scaling parameter of the identity covariance matrix of zero-mean distribution of initial state and initial state estimate\n",
    "        - $x_0, \\hat{x}_0 \\sim \\mathcal{N}(\\mathbf{0}_{n}, \\mathbf{I}_n$ * *X0_scaling* $)$\n",
    "        - If *test_model* is *statistics_pointdistribution_openloop*\n",
    "            $x_0 = V_i$ * *X0_scaling* for i$^{th}$ orthonormal eigenvector of dynamics $A$\n",
    "    - ***multiprocessing*** : default set to **True** for *statistics* experiments uses *concurrent.futures.ProcessPoolExecutor()* to run statistical experiments for different models/tests in parallel "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46a529ed63f7e72f"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "90f49da036ffc5eb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run + Plot Instructions\n",
    "\n",
    "Define the experiment parameters in experiment_parameters.csv and set corresponding experiment number to *exp_no*\n",
    "\n",
    "<span style=\"color:red;\">WARNING: Check for unique experiment numbers to prevent overwriting data.</span>\n",
    "Default Flags: *run_flag, plot_flag = False, True*\n",
    "Set *run_flag = True* to regen data and run simulation from scratch\n",
    "Note that most test cases of a single realization may take 5 - 10 minutes, statistical cases may take 10-20 hours with the *multiprocessing* parameter set to **True** "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92d9ef1c5e97187d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Code Flags"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4a11cdb095dfe0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run_flag = False    # Uses pre-generated model data from DataDump folder\n",
    "# run_flag = True   # Simulates experiment and saves data to DataDump folder for plotting\n",
    "\n",
    "plot_flag = True   # Plots data for exp_no in DataDump folder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb21c198ffd9c541"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Numerical Analysis and Experiment Summary"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfe834a066917151"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exp No 1\n",
    "### 1.1: Fixed vs Self-tuning Architecture\n",
    "#### Experiment setup\n",
    "$50$-node undirected, uniformly weighted graph with dynamics $|\\lambda_i (A) - 1| \\leq 0.1$ $\\forall i \\in [1, 2,..., 50]$\n",
    "Architecture constrained by $\\mathcal{L}_m = \\mathcal{L}_M = \\mathcal{L}_{m'} = \\mathcal{L}_{M'} = 5$ and no running/switching costs\n",
    "Design-time fixed active architecture set greedy optimized over a $20$-time step prediction horizon\n",
    "Self-tuning architecture initialized from design-time greedy optimal active set, optimized at each simulation step over a receding $10$-time step prediction horizon, unrestricted $N\\rightarrow\\infty$ number of iterations with $N^'=2$ changes to active sets per selection/rejection subsequences\n",
    "#### Code"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f010d6d373d4f15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp_no = 1\n",
    "ff.run_experiment(exp_no, run_check=run_flag, plot_check=plot_flag)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b8b7fb6b24f0c77"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 : Statistics of Fixed vs Self-tuning Architecture for different graphs\n",
    "#### Experiment setup\n",
    "Statistics of $100$-iterations of Exp 1.1 where the network graph is completely randomized\n",
    "#### Plot"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "359d33bc7b25a751"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp_no = 2\n",
    "ff.run_experiment(exp_no, run_check=run_flag, plot_check=plot_flag)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78225e193704d623"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 : Statistics of Fixed vs Self-tuning Architecture for state-space exploration\n",
    "#### Experiment setup\n",
    "Statistics of $50$-iterations of Exp 1.1 where the network graph is fixed and true state is initialized on the $i^{th}$ eigenvector (i.e. normalized eigenvector or unit vector along the $i^{th}$ eigenvector)\n",
    "#### Plot"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f078072b0090b0f2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp_no = 3\n",
    "ff.run_experiment(exp_no, run_check=run_flag, plot_check=plot_flag)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92651e88c501789f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Discussion\n",
    "|         Plot         | Fixed Architecture                                | Self-tuning Architecture                                                                             |\n",
    "|:--------------------:|:----------------------------------------------------------|:-----------------------------------------------------------------------------------------------------|\n",
    "|      Cost Plots      | Fails to stabilize the system - predicted and estimated costs increase unbounded                                                                    | Costs converge and system is stabilized within disturbance tolerances                                |\n",
    "|      Trajectory      | States increase unbounded and relative magnitude of prediction error marginally stabilises - Suggests sufficient sensors and insufficient actuators | States, state estimates and error converges within disturbance tolerances                            |\n",
    "|     Architecture     | Fixed locations and active set size by problem definition                                                                                           | Fixed active set size by problem definition. Greedy optimal change to architecture at each time step |\n",
    "|     Compute time     | Only calculates system update parameters at each time step - $ms$/$\\mu s$ scale computation                                                        | $\\sim 10^1$ seconds per time step to evaluate greedy-optimal architecture                            |\n",
    "\n",
    "Across all true initial states (and as highlighted by the sample lines), self-tuning architecture consistently performs better than fixed architecture. The tests across the state-space of a network and on different networks reinforce this.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcd4490a624be7ab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exp No 2\n",
    "## 2.1 : Self-tuning Architecture for number of changes per iteration of the algorithm\n",
    "### Experiment setup\n",
    "$50$-node undirected, uniformly weighted graph with dynamics $|\\lambda_i (A) - 1| \\leq 0.1$ $\\forall i \\in [1, 2,..., 50]$\n",
    "Architecture constrained by $\\mathcal{L}_m = \\mathcal{L}_M = \\mathcal{L}_{m'} = \\mathcal{L}_{M'} = 5$ and no running/switching costs\n",
    "Self-tuning architecture initialized from design-time greedy optimal active set, optimized at each simulation step over a receding $10$-time step prediction horizon\n",
    "Comparison of $N\\rightarrow\\infty$ (unrestricted) to $N=1$ changes per active set per iteration, both with $N^'=2$ changes to active sets per selection/rejection subsequences\n",
    "### Plot"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e37ed24adbe31cbd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp_no = 4\n",
    "ff.run_experiment(exp_no, run_check=run_flag, plot_check=plot_flag)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "756811d6071824a8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Discussion\n",
    "By nature of the greedy algorithm, increasing $N=1$ to $N\\rightarrow\\infty$ has diminishing returns on the improvement of the architecture. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbb5690c00bb8dfd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 : Statistics of Self-tuning Architecture for number of changes per iteration of the algorithm\n",
    "### Experiment setup\n",
    "Statistics of $100$-iterations of Exp 2.1 where the network graph is completely randomized\n",
    "### Plot"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b848403be788ceb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp_no = 5\n",
    "ff.run_experiment(exp_no, run_check=run_flag, plot_check=plot_flag)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1d248b373753b64"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Discussion\n",
    "By nature of the greedy algorithm, increasing $N=1$ to $N\\rightarrow\\infty$ has diminishing returns on the improvement of the architecture. The overlapping cost regions show that if $N\\rightarrow\\infty$, cumulative cost reduces and computation time increases by additional optimizing iterations compared to $N=1$.  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ea63e92c6ab4d53"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exp No 3\n",
    "## 3.1 : Self-tuning Architecture for length of prediction horizon\n",
    "### Experiment setup\n",
    "$50$-node undirected, uniformly weighted graphs with dynamics $|\\lambda_i (A) - 1| \\leq 0.1$ $\\forall i \\in [1, 2,..., 50]$\n",
    "Architecture constrained by $\\mathcal{L}_m = \\mathcal{L}_M = \\mathcal{L}_{m'} = \\mathcal{L}_{M'} = 5$ and no running/switching costs\n",
    "Self-tuning architecture initialized from design-time greedy optimal active set, optimized at each simulation step over a receding $5$-time step prediction horizon, $N=1$ number of changes per iterations with $N'=1$ change to each active sets per selection/rejection subsequences\n",
    "Comparison of $T_p=1$ to $5T_p$ changes per active set per iteration, both with $N^'=2$ changes to active sets per selection/rejection subsequences\n",
    "### Plot"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b27171b6157e401e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp_no = 6\n",
    "ff.run_experiment(exp_no, run_check=run_flag, plot_check=plot_flag)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18076bb749e95eb5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 : Self-tuning Architecture for length of prediction horizon\n",
    "### Experiment setup\n",
    "Statistics of $100$-iterations of Exp 3.1 where the network graph is completely randomized"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c92c22f98aa7df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp_no = 7\n",
    "ff.run_experiment(exp_no, run_check=run_flag, plot_check=plot_flag)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a02fb5778a01dbb2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Discussion\n",
    "The overlapping cost regions show that if $N\\rightarrow\\infty$, cumulative cost reduces and computation time increases by additional optimizing iterations compared to $N=1$. However, there are diminishing returns by nature of the greedy algorithm. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca3f4445be9beaf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exp No 4\n",
    "## 4.1 : Self-tuning architecture with running and switching costs\n",
    "### Experiment setup\n",
    "$50$-node undirected, uniformly weighted graphs with dynamics $|\\lambda_i (A) - 1| \\leq 0.1$ $\\forall i \\in [1, 2,..., 50]$\n",
    "Self-tuning architecture initialized from design-time greedy optimal active set, optimized at each simulation step over a receding $5$-time step prediction horizon, $N=1$ number of changes per iterations with $N'=1$ change to each active sets per selection/rejection subsequences\n",
    "Comparison of $T_p=5$ to $2T_p$ changes per active set per iteration, both with $N^'=2$ changes to active sets per selection/rejection subsequences\n",
    "### Plot"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe1afe6787f157fc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp_no = 8\n",
    "ff.run_experiment(exp_no, run_check=run_flag, plot_check=plot_flag)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54ec3872ad313b8e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 : Self-tuning Architecture for length of prediction horizon\n",
    "### Experiment setup\n",
    "Statistics of $100$-iterations of Exp 3.1 where the network graph is completely randomized"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b26b40dbfbb9663"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp_no = 9\n",
    "ff.run_experiment(exp_no, run_check=run_flag, plot_check=plot_flag)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1cf8a1926882a1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 : Self-tuning Architecture for length of prediction horizon\n",
    "### Experiment setup\n",
    "Statistics of $100$-iterations of Exp 3.1 where the network graph is completely randomized"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e98f770c1ac88fde"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp_no = 9\n",
    "ff.run_experiment(exp_no, run_check=run_flag, plot_check=plot_flag)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5067ac511be60a75"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Discussion\n",
    "The overlapping cost regions show that if $N\\rightarrow\\infty$, cumulative cost reduces and computation time increases by additional optimizing iterations compared to $N=1$. However, there are diminishing returns by nature of the greedy algorithm. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98fcbb4a4224d3a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
